# NTHPDA Rust for HPC Applications

## TODO
- [ ] ref the repo in the pdf

## Structure Slides

- Motivation:
    - What can you do about this topic
        - Last year somebody did a taxonomy of Rust HPC projects
        - Instead, I show how to actually do highly performant computing in Rust
            - Problem based learning
                - One learns techniques when they are relevant and useful, just in time
        - Problem based learning is difficult. The example has to be
            - Small enough to fit in the scope of a report
            - Complex enough to surface most of the concepts used in actual performance engineering
            - interesting enough to keep readers engaged in the topic
        - Motivate Matrix Multiplication
            - The core of all ML
                - DL wird immer groesser
                - somit immer mehr Matmuls
                - explizites Beispiel: Wie viele finden bei einer inference statt
                - paper raussuchen
- Introduction Rust:
    - Rust
        - Initially released by Mozilla Research in 2015
        - Designed as a Memory Safe System Language as an Alternative for C++ in Servo
            - Servo is the web rendering engine used in Firefox
            - How based unsafe memory is 
                - <https://alexgaynor.net/2020/may/27/science-on-memory-unsafety-and-security/>
                - cite subthings
            - Even Chromium uses it it now
                - <https://security.googleblog.com/2023/01/supporting-use-of-rust-in-chromium.html>
        - Adopted by many big tech firms such as
            - Amazon
                - <https://aws.amazon.com/de/blogs/opensource/why-aws-loves-rust-and-how-wed-like-to-help/>
            - Google
                - <https://google.github.io/comprehensive-rust/>
            - Meta
                - <https://engineering.fb.com/2022/07/27/developer-tools/programming-languages-endorsed-for-server-side-use-at-meta/>
            - Microsoft
                - <https://www.zdnet.de/88403712/microsoft-distanziert-sich-von-c-und-c/>
        - In december 2022, it became the first language other than C and Assembly supported for Linux kernel development
    - Why Rust is a good fit for HPC
        - Its like modern C++ enforced by the Compiler
            - RAII-based memory management
            - References are like `std::unique_ptr`
        - Great Python/C++ interoperability
            - Search out the Libraries
        - Allows for very low level control; even supports bare mental deployment
            - Low Level
                - No Garbage Collection
                - Custom Memory Allocators
                - Raw Pointer Arithmetic in unsafe code
                - Platform specific code with conditional compilation
            - bare metal:
                - explain the structure of core alloc std
                - do some more research
        - Mature compiler optimizations through LLVM backend
            - Explain how the FE -> IR -> BE -> Assembly works
        - Many modern concepts from functional programming
            - Immutability by default
            - Flat Traits/typeclasses instead of deep inheritance
            - Exhaustive pattern matching, resulting in no implicitly unhandled cases
            - AGTs, allowing for easier pattern matching with enumerable sum types
            - No Nullability
                - Instead Option, Result that must be handled and can't be ignoed
                - billion dollar mistake tony hoare
                    - <https://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare/>
        - Developers most loved language for the 7th year according to Stackoverflow
- Formal Introduction of Matrix Multiplication
    - Take Formula from wikipedia
    - picture from slide
    - mention the dot product fun fact
- Structure
    - Chapter 2
        - Simplified Version: Fixed size matmul
        - Where the focus will be on
            - Microbenchmarking
            - Full Application Benchmarking
            - Assembly Analysis
            - Loop unrolling and function inlining
    - Chapter 3
        - Variadic size matrix multiplication
        - Where the focus will be on
            - Profiling
            - Compiler Optimizations
            - Cache Oblivious Algorithms
            - Benchmarking in noisy environments
    - Chapter 4
        - Introduction to Parallelism
        - Where the focus will be on
            - SIMD
            - Multithreading using `rayon`
    - Chapter 5
        - Conclusion
            - Providing an overview of all shown tools
            - Providing further ressources
---
Chapter 2: Fixed Size 3x3 mat
- Naive implementation
    - Show implementation
    - This is call by value
    - Intuitively, this could be improved by using call-by-reference
    - But it is always better to measure than to just guess
    - Therefore, we have to do some Benchmarking. We can do
        - either Microbenchmarking
        - or Full Application Benchmarking
- Microbenchmarking
    - Is the performance evaulation of small isolated functions
    - We will look at two solutions
    1. Native Benchmarking: `cargo bench`
        - Not stable, nightly only
        - No integrated regression testing or visualizations
        - No clear roadmap to become stable
            - cite see slide
        - `cargo-benchcmp` for comparing benchmarks
    2. The canonical benchmarking library: `criterion.rs`
        - based on haskells criterion
        - Uses statistical analysis for regression significance
        - Blocks constant folding using `criterion::black_box`
            - "A function that is opaque to the optimizer, used to prevent the compiler from optimizing away computations in a benchmark."
                - <https://docs.rs/criterion/latest/criterion/fn.black_box.html>
        - HTML reports with plotting through `gnuplot`
        - `cargo-critcmp` for comparing benchmarks
- Full Application Benchmarking
    - Hyperfine
        - Also allows for statistical analysis and outlier detection
        - Supports warmup runs for things such as page cache preparation
        - Can run commands inbetween runs to clear cache: 
            - Example: `echo 1 > /proc/sys/vm/drop_caches` which
                - frees the page cache
                - <https://www.kernel.org/doc/Documentation/sysctl/vm.txt>
        - Supports exports to formats such as JSON or CSV for further analysis
        - Supports parametrized benchmarks
        - Repo contains various python scripts for visualization
            - <https://github.com/sharkdp/hyperfine/tree/master/scripts>
- Performance Optimization
    - Call by Reference
        - Add code as ref in appendix
        - mention that it improved performance by n%
            - full table in appendix
            - redo the benchmarks
        - Transition:
            - We used dynamically sized heap vectors, we know the length
    - Primitive Stack Arrays
        - We first used `std::vec`
            - heap allocated
            - dynamically sized
            - runtime bound checks everywhere
        - Instead, we can just specify that we want a ref to a continuous memory segment of proper size
            - show `&[[f32; 3]; 3]`
        - add code full
        - mention that it improved by n%
            - full table in appendix
        - possible explainations:
            - A lot of non-zero cost work initializing the complex vec struct
            - heap allocations
            - more conditional checks
                - explicitly bounds checking
                    - See the rust bounds check cookbook
                        - <https://github.com/Shnatsel/bounds-check-cookbook/>
            - Footnote:
                - Further analysis can be done through statistical profiling
                - explained later
                - as it doesn't help explaining perf eng concepts, it is left as an exercise to the reader
    - Reduce pointer chasing
        - Instead of 3 arrays of length 3, one could use a single array of length 9
        - this would half the number of dereferences
        - add code full
        - mention that it improved by n%
            - full table in appendix
    - Transition: This code is small enough that one could still analyze the assembly for further optimization
- Assembly Optimizations
    - How Assembly can be analyzed
        - Compiler Explorer
            - Online development environment
            - Started in 2012 for optimizing quantitative analysis algorithms
            - Supports
                - Over 30 Languages such as C/C++/Rust, but also Java/Python/Haskell
                - Supports customized compiler arguments (for example comparing `-O3` and `-Osize`)
                - Different compilers and versions for the same languages (i.e. `gcc` or `clang` for C)
            - Can be used either hosted or on-prem
            - Great UX, color codes the lines to map code to assembly
            - best fit for small programs as it can only handle a single file
        - `cargo-show-asm`
            - More minimalist, less polished CLI tool
            - Can work with any rust codebase, no matter the size or dependencies
            - Allows to view Assembly or LLVM-IR
            - Can query single functions
                - Can also resolve trait implementations
                    - which are roughly the equivalent of interface implementations in OOP
    - Loop Unrolling
        - Explaination:
            - If you know how often the loop runs, just replicate the assembly that many times
                - Pro:
                    - Reduces comparisons and jumps
                    - Easier pipelining; no need for path prediction (which can be wrong)
                - Contra:
                    - increases binary size
        - Was already applied in our case
        - Tooling: `unroll` provides a macro for creating unrolled rust code using the preprocessor
            - As it literally just duplicates the rust code, it does not work for dynamic length loops (although they can be deduced at compile time)
        - For those: `-C llvm-args=-unroll-threshold=N`
            - Do not apply without benchmarking, as it can consume too much of the CPU instruction cache.
    - Function Inlining
        - Explaination:
            - Instead of jumping into a subroutine, and then back, it places the assembly of the subroutine into the outer function
                - Pro:
                    - Eliminates call overhead (moving arguments into registers)
                        - How much call overhead is based on the calling convention used
                            - Rust has no specified default calling convention
                - Contra:
                    - Increases binary size through code duplication
                    - Maybe worse performance through worse CPU instruction cache utilization
        - Was not applied in our case
        - But compiler hints exist: `#[inline(/always/never)]`
---
Chapter 3: Variadic Size Matrix Multiplication
- Lets say, you get the following task:
    - "bla bla here is a deep learning framework that runs too slow, please optimize xoxo"
    - Now the questions are
        - "Why is it so slow"
        - "How does one figure this out?"
    - Solution: Profiling
- Profiling:
    - cite <https://nnethercote.github.io/perf-book/profiling.html> for enumeration
    - Profiling is used to find out which parts of the program are executed frequently enough to affect runtime
    - Since Rust produces normal binaries, most profilers just work
        - Including `perf` and `cachegrind`
    - Since Rust allows for polymorphism, the name mangling occurs. If the profiler doesn't support unmangling natively, `rustfilt` can be used instead
    - In this chapter, both `cargo-flamegraph` and `iai` will be represented
- Cargo flamegraph:
    - Statistical profiler
        - interrupts randomly
        - looks at the stack
        - then, using a monte carlo approach, it can approximate how much time is spent in each function
    - Uses `perf` and `dtrace` internally
    - Result: It is a slow `nxn` matrix multiplication! For the benchmarks, we assume `n=1024`.
- Show unoptimized code
- We can apply our knowledge from the previous chapter
    - Show optimized code
- Before measuring, lets also tweak the whole project
- Compiler Optimizations:
    - Biggest Point: Use a release build (i.e. `-O3`)
        - This enables general optimization as well as vectorization
    - Enable LLVM link time optimization
        - further intermodular optimizations done during the link stage
            - Pro: Possible performance increasements
            - Contra: More compile time
    - Compiling for the native architecture
        - This allows to use more specialized instructions that are not available on every processor such as bigger vector registers for SIMD.
    - Using a single LLVM Codegen unit
        - This means to fully compile the whole program every time (excluding its dependencies)
            - Pro: More possibility to do global compiler optimizations
            - Contra: More compile time
    - Lastly, Profile Guided Optimization (PGO) was used. This is out of the scope for this project, see LINK for a great introduction on how the compiler team used it on `rustc` themselves.
        - <https://blog.rust-lang.org/inside-rust/2020/11/11/exploring-pgo-for-the-rust-compiler.html>
- Results table, graphs
- Cache-oblivious algorithms
    - use pic from slide
    - Standard Matrix Multiplication `A \cdot B`
        - A traverses row-major order
        - B traverses column-major order
            - Every step of B we get a cache miss
        - Solution: Transpose B
        - `C_{ij}` becomes row `A_i` times **row** `B_j`
        - Requires `\Theta(n^2)` precompute
    - Two questions:
        1. Does it improve speed?
            - Just benchmark it.
            - Yes, by n% (mean/median)
        2. Does it reduce cache misses?
            - For that, we can use `iai`, which is also very useful to benchmark in noisy environments
- `Iai`
    - `iai` is a high-precision, one-shot benchmark framework for Rust code.
    - This means that the code is run only a single time
    - It uses Cachegrind to simulate emulate the CPU and its caches
    - Thus, it is not affected by the noisy-neighbour problem facing traditional CI benchmarks
    - Show results
    - ALthrough it somehow has worse RAM, better L1 and L2
        - again: Analyzing that is beyond the scope and doesnt help explaining the concepts, thus exercise for the reader
---
Chapter 4: A glimpse of (Inter-Node) Parallelism
- Lastly, we are showing 2 kinds of inter-node parallism
    - Single thread parallelism using SIMD instructions
    - Multi threading using Rayon
- SIMD
    - TODO explain SIMD
    - part of rust core library
    - two different APIs: The old, processor specific API and the new portable SIMD API
    - Old API
        - Experimental only: unsafe code
        - Low level platform specific structs
        - direct intrinsics translation
        - show how the functions look the same, see slide
        - Part of `core`: Doesn't require any allocator or OS features existing
    - Portable SIMD
        - Experimental only, SAFE
        - Generalized on bit width level
            - `std::simd::{f32x8, f64x4, i32x8}`
        - In order to not crash: Conditional compilation with
            - `#[cfg(target_arch="x86_64")]`
            - `#[cfg(target_feature="aes")]`
        - On runtime: Conditional execution with `std::is_x86_feature_detected`
        - If not checked: undefinied behvaiour
        - Part of `std`: Expects a global memory allocator as well as an operating system.
- Multithreading
    - Rust supports many ways of multi threading
        - Primitives using the standard libary
            - See the book for More
                - <https://doc.rust-lang.org/book/ch16-00-concurrency.html>
        - Furthermore, `async/await` for managing async I/O
            - In order to enable the usage in many different environments (such as IoT), rust requires the developer to bring their own async runtime
            - The most common one is tokio
            - Other examples are `smol` as a simpler solution or `fuchsia-async` used in Googles Fuchsia OS>
            - For more on Rust Async I/O, see
                - "Asynchronous Programming in Rust"
                    - <https://rust-lang.github.io/async-book/01_getting_started/01_chapter.html#getting-started>
                - fasterthanlimes "Understanding Rust futures by going way too deep"
                    - <https://fasterthanli.me/articles/understanding-rust-futures-by-going-way-too-deep>
                - WithoutBoats "Zero-Cost Async IO" talk at RustLatam 2019
                    - <https://www.youtube.com/watch?v=skos4B5x7qE>
        - Lastly, there are many utility libraries. 
            - Here, we will focus on `rayon`
    - Rayon
        - Rayon is a High-Level Parallelism Library, using dynamically sized thread pools
        - It gurantees **data-race freedom** by only allowing one thread to write
        - Its main features are: Parallel Iterators
            - Just replace `.iter()` with `.par_iter()`
            - Same functionality as sequential **if** the iterator has no side effects
            - Support for common high level functions such as
                - `.map(), .filter(), .reduce()...`
                - By implementing the iterator trait
            - Low level primitives such as `join()` enabling the `fork-join` model
                - `.join(|| a(), || b())`
                - **May** run in parallel based on whether idle cores are available
        - Example:
            - Show code
            - When writing it more functional...
            - one can only replace the `iter_mut()` by `.par_iter_mut()`
---
Chapter 5: Conclusion
