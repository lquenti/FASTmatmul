\section{Introduction}
\subsection{Motivation}
From a programming language perspective, \ac{HPC} is dominated by code written C, C++ or Fortran as these provide the low level control and optimization capablilites common in tightly-optimized code. However, as Rust was initially designed as a modern, memory-safe C++ replacement, it could be a valid choice for any kind of performance-critical code.\\

Instead of just providing yet another taxonomy of successful \ac{HPC} projects in Rust, this report will rather provide an introduction to the topic on performance engineering in Rust, implicitly covering the current state of the ecosystem while providing an short explaination of each of the common concepts. Instead of just providing an enumeration of techniques, it rather uses \ac{PBL} to introduce the techniques just-in-time when they are relevant, providing a more coherent learning progression.\\

\acl{PBL} can be difficult. The problem has to be
\begin{itemize}
  \item Small enough to fit the scope of a report
  \item Complex enough to cover most of the concepts of real-life performance engineering
  \item Interesting enough to keep readers engaged in the topic
\end{itemize}

For this report, we decided to analyze matrix multiplications. More than just a toy-problem, the matrix multiplication is at the core of all deep learning frameworks. As the parameter count steadily increases into the trillions \cite{gpt4}, fast matrix multiplications become evermore important for todays frameworks.

\subsection{Rust}
Rust \cite{rust} is a systems programming language initially released by Mozilla Research in 2015. It was designed as a memory safe alternative for C++ in Servo \cite{servo}, which is the web rendering engine used in Firefox. Rusts main goal is to provide memory safety while having an on-par performance with other systems languages such as C or C++.\\

Having memory safety is paramount, as most security issues in traditional C/C++ codebases are a result of using a memory-unsafe language. To quote the overview by Alex Gaynor \cite{gaynor}


\begin{itemize}
  \item Android \cite{androidmem}: "Our data shows that issues like use-after-free, double-free, and heap buffer overflows generally constitute more than \textit{65\%} of High \& Critical security bugs in Chrome and Android."
  \item Android’s bluetooth and media components \cite{androidmem2}: "Use-after-free (UAF), integer overflows, and out of bounds (OOB) reads/writes comprise \textit{90\%} of vulnerabilities with OOB being the most common."
  \item iOS and macOS \cite{iosmem}: "Across the entirety of iOS 12 Apple has fixed 261 CVEs, 173 of which were memory unsafety. That’s \textit{66.3\%} of all vulnerabilities." and "Across the entirety of Mojave Apple has fixed 298 CVEs, 213 of which were memory unsafety. That’s \textit{71.5\%} of all vulnerabilities."
  \item Chrome \cite{chromemem}: "The Chromium project finds that around \textit{70\%} of our serious security bugs are memory safety problems."
  \item Microsoft \cite{microsoftmem}: \textit{"$\sim$70\%} of the vulnerabilities Microsoft assigns a CVE each year continue to be memory safety issues"
  \item Firefox’s CSS subsystem \cite{ffmem}: "If we’d had a time machine and could have written this component in Rust from the start, 51 (\textit{73.9\%}) of these bugs would not have been possible."
  \item Ubuntu’s Linux kernel \cite{ubumem}: "\textit{65\%} of CVEs behind the last six months of Ubuntu security updates to the Linux kernel have been memory unsafety."
\end{itemize}

Furthermore, it is now adapted by many big tech firms such as Amazon \cite{awsrust}, Google \cite{googlerust}, Meta \cite{metarust}, and Microsoft \cite{msftrust}. Lastly, in december 2022, it became the first language other than C and Assembly supported for Linux kernel development \cite{linuxrust}.

\subsubsection{Why Rust is a good fit for HPC}
Basically, one can think of Rust as a modern dialect of C++ enforced by the compiler. It uses \ac{RAII} internally to ensure memory safety, while references are roughly equivalent to \texttt{std::unique\_ptr}.\\

Especially relevant is the great interoperability with other languages. It supports easy integration with C++ using \texttt{bindgen} \cite{bindgen}, which is developed by the Rust core team. Rust also allows for easy embedding into Python code using \texttt{PyO3} \cite{pyo3}, allowing for high-performant native extensions.\\

Furthermore, it allows for very low level control, even to the extend of bare metal deployment support. Due to Rusts aforementioned \ac{RAII}-alike memory management model, the runtime has no need for a garbage collector. One can even bring their own memory allocator and do raw pointer arithmetic if required. Lastly, Rust supports architecture based conditional compilation which makes it possible to write fast programs leveraging modern CPU instructions while providing portable alternatives. To support bare metal, OS-less development, Rust's standard library is split into 3 tiers:

\begin{itemize}
  \item \texttt{core}: The \texttt{core} library provides essential types and functionality that do not require heap memory allocation.
  \item \texttt{alloc}: The \texttt{alloc} library builds upon the \texttt{core} library but expects heap-allocations, thus supporting things such as dynamically sized vectors.
  \item \texttt{std}: The \texttt{std} library is the highest-level tier, requiring not only a memory allocator but also several OS capablilites such as I/O management.
\end{itemize}

Although Rust itself is a relatively new language, its compiler supports most modern compiler optimizations. This is possible through \ac{LLVM}. Instead of producing native assembly for all architectures, the compiler just provides a \ac{LLVM} frontend generating \ac{LLVM} \ac{IR} which then gets translated to native code by \ac{LLVM}.\\

Lastly, it supports many modern functional concepts such as immutability by default, flat traits instead of deep inheritance, exhaustive pattern matching with \acp{ADT} sum types as well as providing alternatives to nullability, which is commonly known as the billion dollar mistake \cite{null}. Its language design is in fact so popular that according to the yearly StackOverflow surveys it was voted as the most loved language for the 7th year in the row \cite{sosurvey}.

\subsection{Quadratic Matrix Multiplication}

Let $A, B \in \mathbb{R}^{n\times n}, n \in \mathbb{N}$. Then $C \in \mathbb{R}^{n \times n}$ is defined as
\[
  C_{ij} := \sum_{k=1}^n A_{ik} \cdot B_{kj}.
\]
$C_{ij}$ is the dot product of the $i$-th row of $A$ and the $j$-th column of $B$.

\subsection{Structure}
This report is structured as follows: Section 2 will explore a simplified version of the matrix multiplication problem where the dimension is fixed. Here, the focus will be set on microbenchmarking, full application benchmarking, and assembly analysis. Section 3 will then explore the full matrix multiplication, exploring the topics of profiling, compiler optimizations, cache oblivious as well as how to benchmark in noisy environments. Section 4 will provide a short introduction of parallelism. Lastly, section 5 concludes this report by providing an overview of all shown tools as well as further ressources.

\section{Fixed Size Matrix Multiplication}
To start off with a simplified problem, this section focuses on a fixed quadratic matrix size of $n=3$. Using the mathematical definition, this can be trivially implemented:

\begin{listing}[th]
  \inputminted{rust}{./assets/first_impl.rs}
\caption{Naive implementation of a $3\times3$ matrix multiplication.}
\label{lst:hello}
\end{listing}


% Chapter 2: Fixed Size 3x3 mat
%- Naive implementation
%    - Show implementation
%    - This is call by value
%    - Intuitively, this could be improved by using call-by-reference
%    - But it is always better to measure than to just guess
%    - Therefore, we have to do some Benchmarking. We can do
%        - either Microbenchmarking
%        - or Full Application Benchmarking
\subsection{Microbenchmarking}
%- Microbenchmarking
%    - Is the performance evaulation of small isolated functions
%    - We will look at two solutions
%    1. Native Benchmarking: `cargo bench`
%        - Not stable, nightly only
%        - No integrated regression testing or visualizations
%        - No clear roadmap to become stable
%            - cite see slide
%        - `cargo-benchcmp` for comparing benchmarks
%    2. The canonical benchmarking library: `criterion.rs`
%        - based on haskells criterion
%        - Uses statistical analysis for regression significance
%        - Blocks constant folding using `criterion::black_box`
%            - "A function that is opaque to the optimizer, used to prevent the compiler from optimizing away computations in a benchmark."
%                - <https://docs.rs/criterion/latest/criterion/fn.black_box.html>
%        - HTML reports with plotting through `gnuplot`
%        - `cargo-critcmp` for comparing benchmarks
\subsection{Full Application Benchmarking}
%- Full Application Benchmarking
%    - Hyperfine
%        - Also allows for statistical analysis and outlier detection
%        - Supports warmup runs for things such as page cache preparation
%        - Can run commands inbetween runs to clear cache:
%            - Example: `echo 1 > /proc/sys/vm/drop_caches` which
%                - frees the page cache
%                - <https://www.kernel.org/doc/Documentation/sysctl/vm.txt>
%        - Supports exports to formats such as JSON or CSV for further analysis
%        - Supports parametrized benchmarks
%        - Repo contains various python scripts for visualization
%            - <https://github.com/sharkdp/hyperfine/tree/master/scripts>
\subsection{Performance Optimization}
% maybe a sentence of filler/transition?
\subsubsection{Call By Reference}
%    - Call by Reference
%        - Add code as ref in appendix
%        - mention that it improved performance by n%
%            - full table in appendix
%            - redo the benchmarks
%        - Transition:
%            - We used dynamically sized heap vectors, we know the length
\subsubsection{Primitive Stack Arrays}
%    - Primitive Stack Arrays
%        - We first used `std::vec`
%            - heap allocated
%            - dynamically sized
%            - runtime bound checks everywhere
%        - Instead, we can just specify that we want a ref to a continuous memory segment of proper size
%            - show `&[[f32; 3]; 3]`
%        - add code full
%        - mention that it improved by n%
%            - full table in appendix
%        - possible explainations:
%            - A lot of non-zero cost work initializing the complex vec struct
%            - heap allocations
%            - more conditional checks
%                - explicitly bounds checking
%                    - See the rust bounds check cookbook
%                        - <https://github.com/Shnatsel/bounds-check-cookbook/>
%            - Footnote:
%                - Further analysis can be done through statistical profiling
%                - explained later
%                - as it doesn't help explaining perf eng concepts, it is left as an exercise to the reader
\subsubsection{Reduce Pointer Chasing}
%    - Reduce pointer chasing
%        - Instead of 3 arrays of length 3, one could use a single array of length 9
%        - this would half the number of dereferences
%        - add code full
%        - mention that it improved by n%
%            - full table in appendix
%    - Transition: This code is small enough that one could still analyze the assembly for further optimization
\subsection{Assembly Optimizations}
% maybe a sentence of filler/transition?
\subsubsection{How Assembly can be Analyzed}
% bla bla, two
\paragraph{Compiler Explorer}
%        - Compiler Explorer
%            - Online development environment
%            - Started in 2012 for optimizing quantitative analysis algorithms
%            - Supports
%                - Over 30 Languages such as C/C++/Rust, but also Java/Python/Haskell
%                - Supports customized compiler arguments (for example comparing `-O3` and `-Osize`)
%                - Different compilers and versions for the same languages (i.e. `gcc` or `clang` for C)
%            - Can be used either hosted or on-prem
%            - Great UX, color codes the lines to map code to assembly
%            - best fit for small programs as it can only handle a single file
\paragraph{cargo-show-asm}
%        - `cargo-show-asm`
%            - More minimalist, less polished CLI tool
%            - Can work with any rust codebase, no matter the size or dependencies
%            - Allows to view Assembly or LLVM-IR
%            - Can query single functions
%                - Can also resolve trait implementations
%                    - which are roughly the equivalent of interface implementations in OOP
\subsubsection{Loop Unrolling}
%    - Loop Unrolling
%        - Explaination:
%            - If you know how often the loop runs, just replicate the assembly that many times
%                - Pro:
%                    - Reduces comparisons and jumps
%                    - Easier pipelining; no need for path prediction (which can be wrong)
%                - Contra:
%                    - increases binary size
%        - Was already applied in our case
%        - Tooling: `unroll` provides a macro for creating unrolled rust code using the preprocessor
%            - As it literally just duplicates the rust code, it does not work for dynamic length loops (although they can be deduced at compile time)
%        - For those: `-C llvm-args=-unroll-threshold=N`
%            - Do not apply without benchmarking, as it can consume too much of the CPU instruction cache.
\subsubsection{Function Inlining}
%    - Function Inlining
%        - Explaination:
%            - Instead of jumping into a subroutine, and then back, it places the assembly of the subroutine into the outer function
%                - Pro:
%                    - Eliminates call overhead (moving arguments into registers)
%                        - How much call overhead is based on the calling convention used
%                            - Rust has no specified default calling convention
%                - Contra:
%                    - Increases binary size through code duplication
%                    - Maybe worse performance through worse CPU instruction cache utilization
%        - Was not applied in our case
%        - But compiler hints exist: `#[inline(/always/never)]`
\section{Variadic Size Matrix Multplication}
%- Lets say, you get the following task:
%    - "bla bla here is a deep learning framework that runs too slow, please optimize xoxo"
%    - Now the questions are
%        - "Why is it so slow"
%        - "How does one figure this out?"
%    - Solution: Profiling
\subsection{Profiling}
%- Profiling:
%    - cite <https://nnethercote.github.io/perf-book/profiling.html> for enumeration
%    - Profiling is used to find out which parts of the program are executed frequently enough to affect runtime
%    - Since Rust produces normal binaries, most profilers just work
%        - Including `perf` and `cachegrind`
%    - Since Rust allows for polymorphism, the name mangling occurs. If the profiler doesn't support unmangling natively, `rustfilt` can be used instead
%    - In this chapter, both `cargo-flamegraph` and `iai` will be represented
\subsection{Cargo Flamegraph}
%- Cargo flamegraph:
%    - Statistical profiler
%        - interrupts randomly
%        - looks at the stack
%        - then, using a monte carlo approach, it can approximate how much time is spent in each function
%    - Uses `perf` and `dtrace` internally
%    - Result: It is a slow `nxn` matrix multiplication! For the benchmarks, we assume `n=1024`.
\subsection{Applying Previous Knowledge}
%- Show unoptimized code
%- We can apply our knowledge from the previous chapter
%    - Show optimized code
%- Before measuring, lets also tweak the whole project
\subsection{Compiler Optimizations}
%- Compiler Optimizations:
%    - Biggest Point: Use a release build (i.e. `-O3`)
%        - This enables general optimization as well as vectorization
%    - Enable LLVM link time optimization
%        - further intermodular optimizations done during the link stage
%            - Pro: Possible performance increasements
%            - Contra: More compile time
%    - Compiling for the native architecture
%        - This allows to use more specialized instructions that are not available on every processor such as bigger vector registers for SIMD.
%    - Using a single LLVM Codegen unit
%        - This means to fully compile the whole program every time (excluding its dependencies)
%            - Pro: More possibility to do global compiler optimizations
%            - Contra: More compile time
%    - Lastly, Profile Guided Optimization (PGO) was used. This is out of the scope for this project, see LINK for a great introduction on how the compiler team used it on `rustc` themselves.
%        - <https://blog.rust-lang.org/inside-rust/2020/11/11/exploring-pgo-for-the-rust-compiler.html>
%- Results table, graphs
\subsection{Cache-oblivious Algorithms}
%- Cache-oblivious algorithms
%    - use pic from slide
%    - Standard Matrix Multiplication `A \cdot B`
%        - A traverses row-major order
%        - B traverses column-major order
%            - Every step of B we get a cache miss
%        - Solution: Transpose B
%        - `C_{ij}` becomes row `A_i` times **row** `B_j`
%        - Requires `\Theta(n^2)` precompute
%    - Two questions:
%        1. Does it improve speed?
%            - Just benchmark it.
%            - Yes, by n% (mean/median)
%        2. Does it reduce cache misses?
%            - For that, we can use `iai`, which is also very useful to benchmark in noisy environments
\subsection{Iai}
%- `Iai`
%    - `iai` is a high-precision, one-shot benchmark framework for Rust code.
%    - This means that the code is run only a single time
%    - It uses Cachegrind to simulate emulate the CPU and its caches
%    - Thus, it is not affected by the noisy-neighbour problem facing traditional CI benchmarks
%    - Show results
%    - ALthrough it somehow has worse RAM, better L1 and L2
%        - again: Analyzing that is beyond the scope and doesnt help explaining the concepts, thus exercise for the reader
\section{A glimpse of (Inter-Node) Parallelism}
%- Lastly, we are showing 2 kinds of inter-node parallism
%    - Single thread parallelism using SIMD instructions
%    - Multi threading using Rayon
\subsection{SIMD}
%- SIMD
%    - TODO explain SIMD
%    - part of rust core library
%    - two different APIs: The old, processor specific API and the new portable SIMD API
%    - Old API
%        - Experimental only: unsafe code
%        - Low level platform specific structs
%        - direct intrinsics translation
%        - show how the functions look the same, see slide
%        - Part of `core`: Doesn't require any allocator or OS features existing
%    - Portable SIMD
%        - Experimental only, SAFE
%        - Generalized on bit width level
%            - `std::simd::{f32x8, f64x4, i32x8}`
%        - In order to not crash: Conditional compilation with
%            - `#[cfg(target_arch="x86_64")]`
%            - `#[cfg(target_feature="aes")]`
%        - On runtime: Conditional execution with `std::is_x86_feature_detected`
%        - If not checked: undefinied behvaiour
%        - Part of `std`: Expects a global memory allocator as well as an operating system.
\subsection{Multithreading}
%- Multithreading
%    - Rust supports many ways of multi threading
%        - Primitives using the standard libary
%            - See the book for More
%                - <https://doc.rust-lang.org/book/ch16-00-concurrency.html>
%        - Furthermore, `async/await` for managing async I/O
%            - In order to enable the usage in many different environments (such as IoT), rust requires the developer to bring their own async runtime
%            - The most common one is tokio
%            - Other examples are `smol` as a simpler solution or `fuchsia-async` used in Googles Fuchsia OS>
%            - For more on Rust Async I/O, see
%                - "Asynchronous Programming in Rust"
%                    - <https://rust-lang.github.io/async-book/01_getting_started/01_chapter.html#getting-started>
%                - fasterthanlimes "Understanding Rust futures by going way too deep"
%                    - <https://fasterthanli.me/articles/understanding-rust-futures-by-going-way-too-deep>
%                - WithoutBoats "Zero-Cost Async IO" talk at RustLatam 2019
%                    - <https://www.youtube.com/watch?v=skos4B5x7qE>
%        - Lastly, there are many utility libraries.
%            - Here, we will focus on `rayon`
\subsubsection{Rayon}
%    - Rayon
%        - Rayon is a High-Level Parallelism Library, using dynamically sized thread pools
%        - It gurantees **data-race freedom** by only allowing one thread to write
%        - Its main features are: Parallel Iterators
%            - Just replace `.iter()` with `.par_iter()`
%            - Same functionality as sequential **if** the iterator has no side effects
%            - Support for common high level functions such as
%                - `.map(), .filter(), .reduce()...`
%                - By implementing the iterator trait
%            - Low level primitives such as `join()` enabling the `fork-join` model
%                - `.join(|| a(), || b())`
%                - **May** run in parallel based on whether idle cores are available
%        - Example:
%            - Show code
%            - When writing it more functional...
%            - one can only replace the `iter_mut()` by `.par_iter_mut()`
\section{Conclusion and Further Ressources}
%- Overview blabla
%    - Rust is viable for HPC, although many parts are still experimental
%    - There are many possibilities for optimizing Rust code
%        - Never blindly optimize without benchmarking
%- Overview of all Tools
%    - show table
%- Further ressources
%   - also mention the repo
%    - Besides the ressources mentioned
%        - see the slide
